---
title: "10. Unsupervised Learning"
output: github_document
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = T, fig.align = "center")
```

Unsupervised: sample response doesn't exist, don't set specific variable

- PCA(unsupervised) vs. PCR(supervised), ofcourse PLS(supervised)
- PCA: loading vector(eigen vector), score vector(scaled with loading vector)
- it MUST DO standardization before PCA. Sum of square of loading vector is 1, as expression of unit vector
- first loading vector a little related with linear regression(or LDA)
- PLA has certain limitation, if unsupervised
<hr />

- K-Means Clustering: k is the number of subgroups, minimize inner-variance grouping
- draw randomly, repeat (find centroid(like perfect center), re-draw color nearest centroid)
- Hierarchical Clustering: tree-like visual representation
- linkage: Complete(maximal), Single(minimal), Average(mean), Centroid and so forth
- Euclidean distance, Correlation-based distance and so on
- BUT might be unrealistic

### pre-requires
```{r warning = F, message = F}
data(USArrests, package = "datasets")
data(NCI60, package = "ISLR")
```

store unique data sets.
```{r eval = F}
write.csv(USArrests, "USArrests.csv", row.names = T, quote = F)
```

```{r eval = F}
NCI60_ = cbind(NCI60$labs, NCI60$data)
write.csv(NCI60_, "NCI60.csv", row.names = F, quote = F)
```

---

### 1. PCA (for unsupervised, not PCR)

```{r}
states = row.names(USArrests)
names(USArrests)
```

##### medium point check & variance check
```{r}
apply(USArrests, 2, mean)
apply(USArrests, 2, var)
```

```{r message = F}
pr.out = prcomp(USArrests, scale = T)
```
> default: scaled to mean zero

##### detail check
```{r}
names(pr.out)
```

```{r}
pr.out$center
pr.out$scale
pr.out$rotation
```
> rotation: loading vector

```{r}
dim(pr.out$x)
```

```{r fig.width = 8, fig.height = 8}
biplot(pr.out, scale = 0)
```

- overall sign change is possible

```{r message = F}
pr.out$rotation = -pr.out$rotation
pr.out$x = -pr.out$x
```

```{r fig.width = 8, fig.height = 8}
biplot(pr.out, scale = 0)
```

```{r}
pr.var = pr.out$sdev^2
pve = pr.var/sum(pr.var); rm(pr.var)
pve
```

```{r}
par(mfrow = c(1,2))
plot(pve, xlab = "Principal Component", ylab = "Proportion of Variance Explained", ylim = c(0,1), type = 'b')
plot(cumsum(pve), xlab = "Principal Component", ylab = "Cumulative Proportion of Variance Explained", ylim = c(0,1), type = 'b')
```

##### addition, cumsum function check
```{r eval = F}
cumsum() check
a = c(1, 2, 8, -3)
cumsum(a)
```

---

### 2. Clustering

#### K-Means Clustering
```{r message = F}
## set.seed(2)
X = matrix(rnorm(50*2), ncol = 2)
X[1:25, 1] = X[1:25, 1] + 3
X[1:25, 2] = X[1:25, 2] - 4
```

```{r}
km.out = kmeans(X, 2, nstart = 20)
km.out$cluster
```
- perfectly separate

```{r}
plot(X, col = (km.out$cluster + 1), main = "K-Means Clustering Results with K = 2", xlab = "", ylab = "", pch = 20, cex = 2)
```

```{r}
## set.seed(4)
km.out = kmeans(X, 3, nstart = 20)
km.out
```

```{r}
plot(X, col = c(km.out$cluster + 1), main = "K-Means Clustering Results with K = 3", xlab = "", ylab = "", pch = 20, cex = 2)
```

- separate left top, little right middle, right bottom

```{r}
## set.seed(3)
km.out = kmeans(X, 3, nstart = 1)
km.out$tot.withinss
```
> nstart: the number of repeat

```{r}
km.out = kmeans(X, 3, nstart = 20)
km.out$tot.withinss
```
> tot.withinss: total inner-variance

#### Hierarchical Clustering
```{r message = F}
hc.complete = hclust(dist(X), "complete")
hc.average = hclust(dist(X), "average")
hc.single = hclust(dist(X), "single")
```

```{r fig.width = 10, fig.height = 10}
par(mfrow = c(1,3))
plot(hc.complete, main = "Complete Linkage", xlab = "", ylab = "", sub = "", cex = .9)
plot(hc.average, main = "Average Linkage", xlab = "", ylab = "", sub = "", cex = .9)
plot(hc.single, main = "Single Linkage", xlab = "", ylab = "", sub = "", cex = .9)
```

```{r}
cutree(hc.complete, 2)
cutree(hc.average, 2)
cutree(hc.single, 2)
```
alike and differ

```{r}
xsc = scale(X)
plot(hclust(dist(xsc), "complete"), main = "Hierarchical Clustering with Scaled Features", xlab = "", ylab = "", sub = "")
```
아, 나 저 글씨 뒤집는 거 배웠는데 기억이 안 난다.

##### Correlation-based distance
```{r fig.height = 6}
X = matrix(rnorm(30*3), ncol = 3)
plot(hclust(as.dist(1 - cor(t(X))), "complete"), main = "Complete Linkage with Correlation-Based Distance", xlab = "", ylab = "", sub = "")
```

---

### NCI60 Data

```{r message = F}
nci.labs = NCI60$labs
nci.data = NCI60$data
```

```{r}
dim(nci.data)
nci.labs[1:4]
table(nci.labs)
```

#### PCA on the data
```{r message = F}
pr.out = prcomp(nci.data, scale = T)
```

```{r message = F}
Cols = function(vec) {
        cols = rainbow(length(unique(vec)))
        return(cols[as.numeric(as.factor(vec))])
}
```

```{r}
par(mfrow = c(1,2))
plot(pr.out$x[, 1:2], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z2")
plot(pr.out$x[, c(1,3)], col = Cols(nci.labs), pch = 19, xlab = "Z1", ylab = "Z3")
```

- didn't see as similar gene, very messy colorful?

```{r}
summary(pr.out)
## summary(pr.out)$importance
```

```{r}
plot(pr.out)
```

- square of sdev

```{r message = F}
pve = 100 * pr.out$sdev^2/sum(pr.out$sdev^2)
```

```{r fig.width = 10, fig.height = 8}
par(mfrow = c(1,2))
plot(pve, type = 'o', xlab = "Principal Component", ylab = "PVE", col = "lightblue")
plot(cumsum(pve), type = 'o', xlab = "Principal Component", ylab = "Cumulative PVE", col = "lightseagreen")
```

- brown3

elbow point is 7

#### Clustering on the data
```{r message = F}
sd.data = scale(nci.data)
```

```{r fig.width = 10, fig.height = 10}
data.dist = dist(sd.data)
plot(hclust(data.dist), labels = nci.labs, main = "Complete Linkage", xlab = "", ylab = "", sub = "")
plot(hclust(data.dist, "average"), labels = nci.labs, main = "Average Linkage", xlab = "", ylab = "", sub = "")
plot(hclust(data.dist, "single"), labels = nci.labs, main = "Single Linkage", xlab = "", ylab = "", sub = "")
```

```{r message = F}
hc.out = hclust(dist(sd.data))
hc.clusters = cutree(hc.out, 4)
```
> cutree( ) doesn't say cut point?

```{r}
table(hc.clusters, nci.labs)
```

```{r fig.width = 10, fig.height = 8}
plot(hc.out, labels = nci.labs, sub = "", xlab = "")
abline(h = 139, col = "red")
```

```{r}
hc.out
```

```{r}
## set.seed(2)
km.out = kmeans(sd.data, 4, nstart = 20)
km.clusters = km.out$cluster
table(km.clusters, hc.clusters)
```
- hierarchical is un-realistic

```{r message = F}
hc.out = hclust(dist(pr.out$x[, 1:5]))
```

```{r fig.width = 10, fig.height = 6}
plot(hc.out, labels = nci.labs, main = "Hier. Clust. on First Five Score Vectors", sub = "", xlab = "")
## abline(h = 100, col = "red")
```

```{r}
table(cutree(hc.out, 4), nci.labs)
```
- changed lots of things
