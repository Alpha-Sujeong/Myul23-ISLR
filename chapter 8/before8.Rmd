---
title: "8. Tree-Based Methods"
output: github_document
---

```{r setup, include = F}
knitr::opts_chunk$set(echo = T, fig.align = "center")
```

Decision Tree

- Bagging: Bootstrap 기법을 이용해 만든 (자기복제형) tree를 평균적으로 구성함.
- Random Forest: bagging보단 나아간 방법, subset m 또한 randomly
- Boosting: bagging하려고 나눈 데이터에서 첫번째로 tree를 구성하고, 나머진 validation set처럼 사용.


### pre-requires
```{r warning = F, message = F}
library(tree)           ## tree()
library(randomForest)   ## randomForest()
library(gbm)            ## gbm()
data(Carseats, package = "ISLR")
data(Boston, package = "MASS")
```

store unique data sets.
```{r eval = F}
write.csv(Carseats, "Carseats.csv", row.names = T, quote = F)
write.csv(Boston, "Boston.csv", row.names = T, quote = F)
```

---

### 1. (simple) Tree

```{r message = F}
High = ifelse(Carseats$Sales <= 8, "No", "Yes")
Carseats = data.frame(Carseats, High)
```

```{r}
tree = tree(High ~.-Sales, Carseats)
summary(tree)
```

```{r fig.width = 13, fig.height = 10}
plot(tree)
text(tree, pretty = 0)
```
와. 함수는 알아보겠는데 설명으로 쓰인 식을 못 알아듣겠는데.

> text( ): node label display, pretty = 0: category name whole display

```{r}
tree
```
- asterisk means it is a terminal node


##### train, test set split
```{r message = F}
## set.seed(2)
train = sample(1:nrow(Carseats), 200)
test = Carseats[-train,]
High.test = High[-train]
```

```{r}
tree.carseats = tree(High ~.-Sales, Carseats, subset = train)
tree.pred = predict(tree.carseats, test, type = "class")
table(tree.pred, High.test)
## sum(diag(table(tree.pred, High.test)))/200
```

```{r message = F}
## set.seed(3)
cv.carseats = cv.tree(tree.carseats, FUN = prune.misclass)
```
역시 함수가 좋아, cv가 바로 연결되어 있잖아.

```{r}
## names(cv.carseats)
cv.carseats
```
- 값을 보면 dev가 제일 작을 때는 3, 4번째 항
- 이때의 tree size(the number of terminal nodes)는 14와 9
- interpret을 위해 tree는 작을수록 좋기 때문에 9를 선택

```{r}
par(mfrow = c(1, 2))
plot(cv.carseats$size, cv.carseats$dev, type = 'b')
plot(cv.carseats$k, cv.carseats$dev, type = 'b')
```

```{r fig.width = 10}
prune.carseats = prune.misclass(tree.carseats, best = 9)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```
앞서 했던 그래프보단 정갈해졌음.

```{r}
tree.pred = predict(prune.carseats, test, type = "class")
table(tree.pred, High.test)
## sum(diag(table(tree.pred, High.test)))/200
```

##### addition
```{r fig.width = 10}
prune.carseats = prune.misclass(tree.carseats, best = 15)
plot(prune.carseats)
text(prune.carseats, pretty = 0)
```

```{r}
tree.pred = predict(prune.carseats, test, type = "class")
table(tree.pred, High.test)
## sum(diag(table(tree.pred, High.test)))/200
```
와 같으면 안 되는데, 더 작은 예측력이 나와야 하는데 왜 같은 값.

---

### 2. Fitting Regression Trees

train vs. test(-train)
```{r message = F}
## set.seed(1)
train = sample(1:nrow(Boston), nrow(Boston)/2)
```

```{r}
tree.boston = tree(medv ~., Boston, subset = train)
summary(tree.boston)
```
흠. 함수가 바뀐 것인가 train randomly가 너무 너무 잘 된 것인가.

```{r fig.width = 10, fig.height = 6}
plot(tree.boston)
text(tree.boston, pretty = 0)
```
plot이 굉장히 다르게 나온 것 같군요. 뭐라는지 못 알아듣겠어요.

```{r}
cv.boston = cv.tree(tree.boston)
plot(cv.boston$size, cv.boston$dev, type = 'b')
```
흠 제껀 size가 늘수록 dev가 감소하는 모양인데요.

##### 일단 조용히 따라해봅니다.
```{r}
prune.boston = prune.tree(tree.boston, best = 5)
plot(prune.boston)
text(prune.boston, pretty = 0)
```
- 살짝 가지치기 되었습니다.

```{r message = F}
yhat = predict(tree.boston, newdata = Boston[-train,])
boston.test = Boston[-train, "medv"]
```

```{r}
plot(yhat, boston.test)
abline(0, 1)
```
흠 솔직히 이 그림 좀 너무하단 생각이 드는데..
변수 변환을 하거나 더미 변수를 쓰거나 둘 중 하나는 해야 할 것 같은데.

```{r}
mean((yhat-boston.test)^2)
```

---

### 3. Bagging and Random Forests

```{r}
## set.seed(1)
bag.boston = randomForest(medv ~., Boston, subset = train, mtry = 13, importance = T)
bag.boston
```
> mtry = 13: 13개의 예측변수를 사용합니다.

```{r message = F}
yhat.bag = predict(bag.boston, newdata = Boston[-train,])
```

```{r}
plot(yhat.bag, boston.test)
abline(0, 1)
```

```{r}
mean((yhat.bag - boston.test)^2)
```

##### ntree = 25
```{r}
bag.boston = randomForest(medv ~., Boston, subset = train, mtry = 13, ntree = 25)
yhat.bag = predict(bag.boston, newdata = Boston[-train,])
mean((yhat.bag - boston.test)^2)
```

##### p/3
```{r}
## set.seed(1)
rf.boston = randomForest(medv ~., Boston, subset = train, mtry = 6, importance = T)
yhat.rf = predict(rf.boston, newdata = Boston[-train,])
mean((yhat.rf - boston.test)^2)
```
seed에 영향인지 mtry를 작게 한 건데, 원래 randomforest보다 매우 약간 크게 나오네요

```{r}
importance(rf.boston)
```
대강 비슷하게 나오는 게 seed에 문제인 것인가<br />
randomforest의 내부 split 함수가 약간 바뀐 문제인 것 같기도.

```{r}
varImpPlot(rf.boston)
```
흠. 참 알아보기 쉽게 나오네요.

---

### 4. Boosting

```{r}
## set.seed(1)
boost.boston = gbm(medv ~., Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4)
summary(boost.boston)
```
> 중요도 확인하게 sorting되어 나옴.

```{r}
plot(boost.boston, i = "rm")
plot(boost.boston, i = "lstat")
```
- i가 방법쪽을 논해서 그런지 한 번에 안 나온다.


```{r}
yhat.boost = predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```
강의 때도 그랬지만, randomforest보다 더 나은 값으로 나오는데.

```{r message = F}
boost.boston = gbm(medv ~., Boston[train,], distribution = "gaussian", n.trees = 5000, interaction.depth = 4, shrinkage = 0.2, verbose = F)
```
> default: shrinkage = 0.001

```{r}
yhat.boost = predict(boost.boston, newdata = Boston[-train,], n.trees = 5000)
mean((yhat.boost - boston.test)^2)
```
수축 모수(lambda?) 값을 늘렸는데 mse 추정도 늘어버렸다. 분명이 줄어든다고 배운 것 같은데.
